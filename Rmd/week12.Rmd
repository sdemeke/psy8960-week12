---
title: "PSY8960 - Week 12"
author: "Saron Demeke"
date: "2023-06-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Script Settings and Resources
```{r echo=FALSE, results=FALSE, warning=FALSE, message=FALSE}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)
library(rvest)
library(tidytext)
library(tm)
library(qdap)
library(textstem)
library(RWeka)
library(ldatuning)
library(topicmodels)
```


## Data Import and Cleaning

I am not sure if RedditExtractoR is an approach we can use for this project and based on no. 4 on instructions, I am assuming you do not want us using this option so I am opting to create the data set by scraping [old Reddit](https://old.reddit.com/r/IOPsychology/)

First, I downloaded 39 pages of old Reddit to reach back to 1 year of posts on IO Psychology subreddit. After downloading and reading in first page, I used a while loop to extract the next page's URL from each page. I guessed average of ~10 days of posts per page and extracted 39 pages which seemed to be the maximum possible as the 'next' button returned an empty character vector on the 40th page. The 39th page contained posts submitted one year ago so this was sufficient. Lastly, I saved the filename of each downloaded page in a vector and sorted it using `gtools::mixedsort()` so that the numbers embedded in file names are ordered correctly.
```{r}
# #get original page
# download.file("https://old.reddit.com/r/IOPsychology/","../data/riopsych_page1.html")
# html <- read_html("../data/riopsych_page1.html")
# 
# i <- 1
# while(i < 40){
#   next_page <- html %>%
#     html_elements(css=".next-button") %>%  #used Developer Tools/Inspect to get attribute of next page URL
#     html_children() %>%
#     html_attr("href")
# 
#   download.file(next_page,paste0("../data/riopsych_page",i+1,".html"))
#   html <- read_html(paste0("../data/riopsych_page",i+1,".html"))
# 
#   i <- i + 1
#   Sys.sleep(2) #buffer for requesting information
# }

# all_pages <- list.files(path="../data",full.names = T) #vector of all file names in ../data with relative path intact
# all_pages <- gtools::mixedsort(all_pages) #sort in order so that embedded numbers in file names are numerically ordered 
```

Next, I created a function that imports the html in each downloaded file and extracts the desired information into a tibble for a single provided link. I use `lapply()` to get the titles and upvotes across all pages and finally collapse each executed function output into one tibble, `week12_tbl`.
```{r}

# get_titles_upvotes <- function(file){
#   
# riopsych_html <- read_html(file)
# 
# titles <- riopsych_html %>% 
#   html_elements(css='.title.may-blank') %>% 
#   html_text()
# 
# upvotes <- riopsych_html %>% 
#   html_elements(xpath='//div[@class = "score unvoted"]') %>% #
#   html_text() %>% 
#   as.numeric() %>% 
#   replace_na(0)
# 
# 
# tbl <- tibble(
#     title=titles, 
#     upvote=upvotes
#   ) 
# 
# return(tbl)
#   
# }
# 
# tbl_list <- lapply(all_pages, get_titles_upvotes)
# 
# tbl_df <- bind_rows(tbl_list) #975 posts from June 2022 to 2023
# 
# write_csv(tbl_df,"../out/week12_tbl.csv")
```


Read in scraped and saved file.

```{r}
week12_tbl <- read_csv("../out/week12_tbl.csv")
```


Create a corpus called io_corpus_original from the titles in week12_tbl.

```{r}
io_corpus_original <- VCorpus(VectorSource(week12_tbl$title))

```


Create a new lemmatized pre-processed corpus from io_corpus_original called io_corpus by:
removing references to IO Psychology (this is the topic of the reddit, so this isn’t useful – remember to also remove variations on this term and double-check your success in the data),
completing any other beneficial pre-processing steps as appropriate to the dataset.
(Be sure to explain in your comments why you chose the steps you chose and the order you chose for them.)


```{r}

io_corpus_original[[17]]$content #IO PSYCHOLOGIST SPELLED WRONG
io_corpus_original[[120]]$content
io_corpus_original[[40]]$content
io_corpus_original[[291]]$content

# [1] "I’m looking into applying for the masters program near me. One of the requirements is a LOR from a psychology professor. I’ve been out of school since 2017 therefore don’t have any relationship with previous professors. Does anyone have any advice? Feeling kind of stuck with that."
# [1] "SIOP Conference - Activation Code?"
# [1] "Pyschometric experts / IO Pyschologist, how did you switch jobs?"
# [1] "Bi-Weekly /r/IOpsychology Discussion - What have you been reading, and what do you think of it?"

my_stopwords <- c("io psychology","io psych","io psychologist"," io ","io practitioner","iopsychology",
                  "industrial psychology","industrialorganizational psychology","riopsychology","psych","psychology")
custom_stopwords <- c(stopwords("en"),my_stopwords)

#use str_Replace_all to replace single ’ and double ” 

io_corpus <- io_corpus_original %>%
  tm_map(content_transformer(str_replace_all),pattern = "’|“|”", replacement = "'") %>% 
  tm_map(content_transformer(replace_contraction)) %>%
  tm_map(removePunctuation) %>% 
  tm_map(removeNumbers) %>%  
  tm_map(content_transformer(str_to_lower)) %>% 
  tm_map(removeWords, custom_stopwords) %>% 
  tm_map(stripWhitespace) %>% 
  tm_map(content_transformer(lemmatize_strings))


io_corpus[[17]]$content
io_corpus[[120]]$content
io_corpus[[40]]$content
io_corpus[[291]]$content
# [1] "look apply master program near one requirement lor psychology professor school since therefore relationship previous professor anyone advice feel kind stick"
# > io_corpus[[120]]$content
# [1] "siop conference activation code"
# > io_corpus[[40]]$content
# [1] "pyschometric expert io pyschologist switch job"
# > io_corpus[[291]]$content
# [1] "biweekly discussion read think"




#order- replace contraction then lemmatize strings
# "I be look into apply for the master program near me. One of the requirement be a LOR from a psychology professor. I have be out of school since 2017 therefore do not have any relationship with previous professor. do anyone have any advice? feel kind of stick with that."
#opposite order
# [1] "I am look into apply for the master program near me. One of the requirement be a LOR from a psychology professor. I have be out of school since 2017 therefore do not have any relationship with previous professor. do anyone have any advice? feel kind of stick with that."
#lemmatize only
# [1] "I'm look into apply for the master program near me. One of the requirement be a LOR from a psychology professor. I've be out of school since 2017 therefore don't have any relationship with previous professor. do anyone have any advice? feel kind of stick with that."


```

```{r}
compare_them <- function(og_corp,proc_corp,random_index) {
  
  list(
  "Index no" = random_index,  
  "Original Corpus Sample" =  og_corp[[random_index]]$content,
  
  "Cleaned Corpus Sample" = proc_corp[[random_index]]$content
  )
}

compare_them(og_corp = io_corpus_original,
             proc_corp = io_corpus,
             sample(1:length(io_corpus_original),1))

```

Create a bigram DTM called io_dtm. Also create a version of this DTM with sparse terms eliminated called io_slim_dtm. Retain between a 2:1 and 3:1 N/k ratio in the slim DTM.

```{r}
#remove empty entries
io_corpus <- tm_filter(io_corpus, FUN = function(x) { return(nchar(stripWhitespace(x$content)[[1]]) > 0) } )
#returns TRUE 973

myTokenizer <- function(x) { NGramTokenizer(x, 
Weka_control(min=1, max=2)) }

io_dtm <- DocumentTermMatrix(io_corpus,
control = list(
tokenize = myTokenizer
)
)
# <<DocumentTermMatrix (documents: 973, terms: 5159)>>
# Non-/sparse entries: 8608/5011099
# Sparsity           : 100%
# Maximal term length: 37
# Weighting          : term frequency (tf)



io_slim_dtm <- removeSparseTerms(io_dtm, .997)
#original N:k was 973/5159
#slimmed N:k is 973/438 or 2.2

```
LDA Topic extraction

```{r}
library(parallel)
library(doParallel)

local_cluster = makeCluster(detectCores() - 1)   
registerDoParallel(local_cluster)

tuning <- FindTopicsNumber( 
io_dtm, 
topics = seq(2,5,8,10), # or whatever range
metrics = c("Griffiths2004", 
"CaoJuan2009", 
"Arun2010", 
"Deveaud2014"),
verbose = T
)

FindTopicsNumber_plot(tuning)

stopCluster(local_cluster)
registerDoSEQ()

```




LDA Results

```{r}
io_lda <- LDA(
  io_dtm,
  k=5,
  control = list(seed = 24)
)

lda_betas <- tidy(io_lda, matrix="beta")
lda_gammas <- tidy(io_lda, matrix="gamma")

#post. probs. - prob that word belongs to topic
lda_betas %>%
group_by(topic) %>%
top_n(10, beta) %>%
arrange(topic, -beta) %>%
view()


```

Respond to the following questions in comments:
Using the beta matrix alone, what topics would you conclude your final topic list maps onto? (e.g., topic 1, 2, 3…n each reflect what substantive topic construct? Use your best judgment.)
Look at the original text of documents with the highest and lowest probabilities assigned to each document. Do your topic names derived from your interpretation of the beta matrix conceptually match with the content of the original posts? What kind of validity evidence does your answer to this question represent?


Create a wordcloud of io_dtm. Remember to interpret it in a comment.


Create a dataset called final_tbl that contains the contents of topics_tbl plus the upvote count.


Run a statistical (not machine learning) analysis to determine if upvotes differs by topic. Explain your interpretation of the results in a comment (you do not need a Publication section this time).
