---
title: "PSY8960 - Week 12"
author: "Saron Demeke"
date: "2023-06-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Script Settings and Resources
```{r echo=FALSE, results=FALSE, warning=FALSE, message=FALSE}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)
library(rvest)
```

## Data Import and Cleaning

I am not sure if RedditExtractoR is an approach we can use for this project and based on 4) on instructions, I am assuming you do not want us using this option so I am opting to create dataset by scraping [old Reddit](https://old.reddit.com/r/IOPsychology/)

First, I downloaded 39 pages of old Reddit to reach back to 1 year of posts on IO Psychology subreddit. After downloading and reading in first page, I used a while loop to extract the next page's URL from each page. I guessed average of ~10 days of posts per page and extracted 39 pages which seemed to be the maximum possible as the 'next' button returned an empty character vector on the 40th page. The 39th page contained posts submitted one year ago so this was sufficient.
```{r}
# #get original page
# download.file("https://old.reddit.com/r/IOPsychology/","../data/riopsych_page1.html")
# html <- read_html("../data/riopsych_page1.html")
# 
# i <- 1
# while(i < 40){
#   next_page <- html %>%
#     html_elements(css=".next-button") %>%  #used Developer Tools/Inspect to get attribute of next page URL
#     html_children() %>%
#     html_attr("href")
# 
#   download.file(next_page,paste0("../data/riopsych_page",i+1,".html"))
#   html <- read_html(paste0("../data/riopsych_page",i+1,".html"))
# 
#   i <- i + 1
#   Sys.sleep(2) #buffer for requesting information
# }
# 
# # date_submitted <- riopsych_html %>%
# #   html_elements(css="time.live-timestamp") %>% #//*[@id="thing_t3_147crcf"]/div[2]/div[1]/p[2]/time
# #   html_text()
# 
# #STEP 2 - function to apply to each file
# #read each html, save variables
```



Next, I created a function that imports the url in each downloaded file, reads the html source code, and extracts the desired information into a tibble for a single provided link. I use `sapply()` to get the titles and upvotes across all pages and finally collapse each executed function ouput into one tibble, `week12_tbl`.
```{r}

get_titles_upvotes <- function(file){
  
riopsych_html <- read_html(file)

titles <- riopsych_html %>% 
  html_elements(css='.title.may-blank') %>% 
  html_text()

upvotes <- riopsych_html %>% 
  html_elements(xpath='//div[@class = "score unvoted"]') %>% #
  html_text() %>% 
  as.numeric() %>% 
  replace_na(0)


tbl <- tibble(
    title=titles, 
    upvote=upvotes
  ) 

return(tbl)
  
}

all_pages <- list.files(path="../data",full.names = T) #vector of all file names in ../data with relative path intact
all_pages <- gtools::mixedsort(all_pages) #sort in order so that embedded numbers in file names are numerically ordered 

tbl_list <- sapply(all_pages, get_titles_upvotes)
week12_tbl <- bind_rows(tbl_list)

```
