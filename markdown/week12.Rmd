---
title: "PSY8960 - Week 12"
author: "Saron Demeke"
date: "2023-06-14"
output: 
  html_document:
    df_print: paged
knit: (function(input, ...) { 
        rmarkdown::render(input, output_file='../out/week12.html') })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Script Settings and Resources
```{r echo=FALSE, results=FALSE, warning=FALSE, message=FALSE}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)
library(rvest)
library(tidytext)
library(tm)
library(qdap)
library(textstem)
library(RWeka)
library(ldatuning)
library(topicmodels)
```


## Data Import and Cleaning

I am not sure if RedditExtractoR is an approach we can use for this project and based on no. 4 on instructions, I am assuming you do not want us using this option so I am opting to create the data set by scraping [old Reddit](https://old.reddit.com/r/IOPsychology/)

First, I downloaded 39 pages of old Reddit to reach back to 1 year of posts on IO Psychology subreddit. After downloading and reading in first page, I used a while loop to extract the next page's URL from each page. I guessed average of ~10 days of posts per page and extracted 39 pages which seemed to be the maximum possible as the 'next' button returned an empty character vector on the 40th page. The 39th page contained posts submitted one year ago so this was sufficient. Lastly, I saved the filename of each downloaded page in a vector and sorted it using `gtools::mixedsort()` so that the page numbers embedded in file names are ordered correctly.
```{r}
# #get original page
# download.file("https://old.reddit.com/r/IOPsychology/","../data/riopsych_page1.html")
# html <- read_html("../data/riopsych_page1.html")
# 
# i <- 1
# while(i < 40){
#   next_page <- html %>%
#     html_elements(css=".next-button") %>%  #used Developer Tools/Inspect to get attribute of next page URL
#     html_children() %>%
#     html_attr("href")
# 
#   download.file(next_page,paste0("../data/riopsych_page",i+1,".html"))
#   html <- read_html(paste0("../data/riopsych_page",i+1,".html"))
# 
#   i <- i + 1
#   Sys.sleep(2) #buffer for requesting information
# }

all_pages <- list.files(path="../data",full.names = T) #vector of all file names in ../data with relative path intact
all_pages <- gtools::mixedsort(all_pages) #sort in order so that embedded numbers in file names are numerically ordered
```

Next, I created a function that imports the html in each downloaded file and extracts the desired information into a tibble for a single provided link. 
```{r}

get_titles_upvotes <- function(file){

riopsych_html <- read_html(file)

titles <- riopsych_html %>%
  html_elements(css='.title.may-blank') %>%
  html_text()

upvotes <- riopsych_html %>%
  html_elements(xpath='//div[@class = "score unvoted"]') %>% #
  html_text() %>%
  as.numeric() %>%
  replace_na(0)


tbl <- tibble(
    title=titles,
    upvote=upvotes
  )

return(tbl)

}

```


I use `lapply()` to get the titles and upvotes across all pages and collapse each executed function output into one tibble, `week12_tbl`.

```{r}
week12_tbl <- lapply(all_pages, get_titles_upvotes) %>% 
  bind_rows()
#975 posts from June 2022 to 2023
```


Using `VectorSource()` and `VCorpus()` to create a corpus from the titles in week12_tbl.

```{r}
io_corpus_original <- VCorpus(VectorSource(week12_tbl$title))
```




To remove references to IO Psychology, I specified custom stopwords with certain variations on the term. Because I set the `removeWords()` pre-processing to occur after characters are converted to lowercase and punctuation is removed, I don't have to specify certain variations like the all caps 'IO'. Before any pre-processing on words, I first used `str_replace_all` to replace the non-R-friendly curly quotes and long dash marks with single quotes and single dashes. The following steps are to replace abbreviations and replace contractions with the full terms. I specify removing punctuation *after* replacing contractions so as not to end up with unrecognizable contractions like "Im" instead of "I'm". I then specify further steps to remove numbers, convert to lower case, and remove all set and custom stopwords. After removing stopwords, I lemmatize (done in this order so that stemming doesn't render some stopwords unrecognizable by `removeWords()`). The final step is to remove  extra spaces.

```{r}
my_stopwords <- c("io psychology","io psych","iopsychology",
                  "industrial psychology","industrialorganizational psychology","riopsychology","iopsych","io psychologist","io psychologists")
custom_stopwords <- c(stopwords("en"),my_stopwords)

io_corpus <- io_corpus_original %>%
  tm_map(content_transformer(str_replace_all),pattern = "’|“|”", replacement = "'") %>% 
  tm_map(content_transformer(str_replace_all),pattern = "—", replacement = "-") %>% 
  tm_map(content_transformer(replace_abbreviation)) %>% 
  tm_map(content_transformer(replace_contraction)) %>%
  tm_map(removePunctuation) %>% 
  tm_map(removeNumbers) %>%  
  tm_map(content_transformer(str_to_lower)) %>% 
  tm_map(removeWords, custom_stopwords) %>% 
  tm_map(content_transformer(lemmatize_strings)) %>% 
  tm_map(stripWhitespace)

```


To compare original and pre-processed strings, custom function `compare_them()` takes in the two corpora and a randomly sampled index and lists the corresponding item from both documents.
```{r}
compare_them <- function(og_corp,proc_corp,random_index) {
  
  list(
    random_index,
  "Original Corpus Sample" =  og_corp[[random_index]]$content,
  
  "Cleaned Corpus Sample" = proc_corp[[random_index]]$content
  )
}

compare_them(og_corp = io_corpus_original,
             proc_corp = io_corpus,
             random_index = sample(1:length(io_corpus_original),1))

```

Before creating a unigram/bigram DTM, I first filtered out 3 empty entries using `tm_filter()`. The DTM is created from this filtered corpus. Next, I use `removeSparseTerms()` with a sparsity set to `.997`. to create `io_slim_dtm`. A 2.1:1 ratio of documents to terms is retained. I
had first tried creating a bigram-only DTM but applying `removeSparseTerms()` is not able to retain a 2-3:1 ratio so I also included unigrams.

```{r}
#remove empty entries
io_corpus_filt <- tm_filter(io_corpus, FUN = function(x) { return(nchar(stripWhitespace(x$content)[[1]]) > 0) } )
#returns TRUE for all 972, 3 removed

myTokenizer <- function(x) { NGramTokenizer(x, 
Weka_control(min=1, max=2)) }

io_dtm <- DocumentTermMatrix(io_corpus_filt,
control = list(
tokenize = myTokenizer
)
)

io_dtm_tbl <- as_tibble(as.matrix(io_dtm))

io_slim_dtm <- removeSparseTerms(io_dtm, .997)
```



## Analysis 

LDA Topic extraction with parallelization. `FindTopicsNumber()` has an argument to set parallelized processing so no need to additionally call *doParallel* package.

```{r}
tuning <- FindTopicsNumber( 
io_dtm, 
topics = seq(2,15,by=1), 
metrics = c("Griffiths2004", 
"CaoJuan2009", 
"Arun2010", 
"Deveaud2014"),
verbose = T,
control = list(seed = 2023),
mc.cores = 6L #use 6 cores
)

FindTopicsNumber_plot(tuning)

```
My interpretation from `ldatuning` results is that the Deveaud2014 and Arun2010 metrics aren't super informative (no clearly visible minima/maxima). The CaoJuan2009 metric suggests 5 or 6 optimal topic numbers looking at the minima. I set a seed for reproducibility but caution that rerunning with different seeds may have ended up in a different decision on how many topics to extract. I am  not sure how to interpret results from the Griffiths2004 metric which doesn't seem to have an idenitifiable maximum so I am choosing to continue with the optimal numbers based on the CaoJuan2009 results. Between 5 and 6 I am opting for 5 topics for simplicity.



I pass `io_dtm` to the `LDA()` function with number of topics set to 5. To create the specified `topics_tbl`, I extracted the document identifier, topic number, and the probability that the document belongs to that topic from the gamma matrix of the LDA object. To get the original post title, I used `tm_index()` on `io_corpus` to identify the indices of the non-empty entries I retained in `io_corpus_filt` and extracted the original titles for these entries.

```{r}
io_lda <- LDA(
  io_dtm,
  k=5,
  control = list(seed = 2023)
)


topics_tbl <- tidy(io_lda, matrix="gamma") %>% 
  group_by(document) %>%
  top_n(1, gamma) %>%
  slice(1) %>%
  ungroup %>% 
  rename(probability=gamma) %>% 
  mutate(doc_id = as.numeric(document),.keep="unused",.before=topic) %>% 
  mutate(original = week12_tbl$title[tm_index(io_corpus, FUN = function(x) {    return(nchar(stripWhitespace(x$content)[[1]]) > 0) })],.before=topic ) %>% 
  arrange(doc_id)

```


Beta matrix to help answer questions
```{r}
tidy(io_lda, matrix="beta") %>% 
  group_by(topic) %>%
  top_n(10, beta) %>%
  arrange(topic, -beta) #%>%  View
```

Respond to the following questions in comments:

**1.** Based on the beta matrix and the words with highest probabilities for each topic, I would conclude that my final topic lists map onto the following three general substantive areas: 

* Topic 1 - Reading Discussions (top words: think, discussion, read)
* Topic 2 - Job and Career Advice (top words: advice, job, career, work)
* Topic 3 - Career Pathways (some of top words: job, interview, siop, field, consult)
* Topic 4 - Professional Development & Learning (top words: work, people, research, book, learn)
* Topic 5 - Student/Early Career Experiences (top words: school, internship, grad)


**2.** Looking at the original post titles with the highest and lowest probabilities assigned by topic, I think the topic names I assigned conceptually match to the original content of posts, providing some preliminary support for the *content validity* of the LDA results. The posts with the highest probabilities for topic 1 vary but several center around thoughts and discussions on specific readings and topics. For topic 2, the most probable posts are on questions related to jobs/careers such as salary and job board postings. For topic 3, many of the most probable posts seem to be posed by early careers/students and center around different paths in IO. The posts in topic 4 vary but many center around specific queries on timely topics and other questions related to learning development for professionals. The posts for topic 5 tend to focus on grad program questions.


Created a dataset called final_tbl that contains the contents of topics_tbl plus the upvote count which I again extracted from the original dataset using `tm_index` and `io_corpus`.

```{r}
final_tbl <- topics_tbl %>% 
  mutate(upvotes = week12_tbl$upvote[tm_index(io_corpus, FUN = function(x) { return(nchar(stripWhitespace(x$content)[[1]]) > 0) })],
         topic = as.factor(topic)) 
```

Finally, ran a statistical regression analysis to determine if upvotes differs by topic. Based on the results, I conclude that the number of votes does significantly differ by topic (overall F statistic is significant at p < 0.05). The coefficients suggest that topic 2, 3, and 4 have significantly more upvotes than topic 1. For a full list of all comparisons, I ran a Tukey HSD analysis as well. This showed that topics 3 are not significantly more popular than topic 2. Overall, it seems topic 1 and 5 are the least popular which suggests that posts related to regular reading discussions and student questions do not receive the most positive votes. The other three topics are more popular but there does not seem to be one that is most popular which suggests similar levels of popularity across these topics.

```{r}
summary(lm(upvotes~topic,data=final_tbl))
TukeyHSD(aov(upvotes ~ topic, data = final_tbl))
```



## Visualization

The wordcloud of `io_dtm` shows the words with the highest frequencies in the provided DTM and is specified to show at most the top 20 most frequent terms with added color scale that additionally assigns different values by frequency. Based on the I/O subreddit DTM, the most frequent words all relate to the topics most common to I/O, including work, job, career, and research. It is exported in the `fig` folder in the current directory as a PNG.

```{r }
color_pal <- viridisLite::inferno(n=5, direction = -1)

io_dtm_m <- as.matrix(io_dtm)

png(filename="../fig/io_wordcloud.png")

wordcloud::wordcloud(
  words = colnames(io_dtm_m),
  freq = colSums(io_dtm_m),
  max.words = 20,
  colors = color_pal
)

dev.off()

knitr::include_graphics("../fig/io_wordcloud.png")

```

